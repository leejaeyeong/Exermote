{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.lib.io import file_io\n",
    "import argparse\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM, Conv1D, Flatten\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from numpy import array, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as k\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from tensorflow.python.saved_model import builder as saved_model_builder\n",
    "from tensorflow.python.saved_model import tag_constants, signature_constants\n",
    "from tensorflow.python.saved_model.signature_def_utils_impl import predict_signature_def\n",
    "\n",
    "import coremltools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "epochs = 50 \n",
    "\n",
    "batch_size = 100\n",
    "validation_split = 0.2\n",
    "\n",
    "# model parameters\n",
    "dropout = 0.2\n",
    "timesteps = 40\n",
    "timesteps_in_future = 20\n",
    "nodes_per_layer = 32\n",
    "filter_length = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_file='data_classes_4_squats_adjusted.csv', job_dir='leeeeeroooooyyyyyjeeeeeenkins', **args):\n",
    "    parameter_string = 'final_25_classes_4_squats_adjusted' + '_dropout_' + str(dropout) + '_timesteps_' + str(\n",
    "        timesteps) + '_timesteps_in_future_' + str(timesteps_in_future) + '_nodes_per_layer_' + str(\n",
    "        nodes_per_layer) + '_filter_length_' + str(filter_length)\n",
    "    if 'gs://' in job_dir:\n",
    "        logs_path = 'gs://exermotemachinelearningengine' + '/logs/' + parameter_string\n",
    "    else:\n",
    "        logs_path = '.' + '/logs/' + parameter_string\n",
    "    print('-----------------------')\n",
    "    print('Using train_file located at {}'.format(train_file))\n",
    "    print('Using logs_path located at {}'.format(logs_path))\n",
    "    print('-----------------------')\n",
    "\n",
    "    # load data\n",
    "    file_stream = file_io.FileIO(train_file, mode='r')\n",
    "    dataframe = read_csv(file_stream, header=0)\n",
    "    dataframe.fillna(0, inplace=True)\n",
    "    dataset = dataframe.values\n",
    "\n",
    "    X = dataset[:, [\n",
    "        #2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, # Device: xGravity, yGravity, zGravity, xAcceleration, yAcceleration, zAcceleration, pitch, roll, yaw, xRotationRate, yRotationRate, zRotationRate\n",
    "        2, 3, 4, 5, 6, 7,  # xAcceleration, yAcceleration, zAcceleration, xRotationRate, yRotationRate, zRotationRate\n",
    "        # 14,15,16,17,                          # Right Hand: rssi, xAcceleration, yAcceleration, zAcceleration\n",
    "        # 18,19,20,21,                          # Left Hand: rssi, xAcceleration, yAcceleration, zAcceleration\n",
    "        # 22,23,24,25,                          # Right Foot: rssi, xAcceleration, yAcceleration, zAcceleration\n",
    "        # 26,27,28,29,                          # Left Foot: rssi, xAcceleration, yAcceleration, zAcceleration\n",
    "        # 30,31,32,33,                          # Chest: rssi, xAcceleration, yAcceleration, zAcceleration\n",
    "        # 34,35,36,37                           # Belly: rssi, xAcceleration, yAcceleration, zAcceleration\n",
    "    ]].astype(float)\n",
    "    y = dataset[:, 0]  # ExerciseType (Index 1 is ExerciseSubType)\n",
    "\n",
    "    # data parameters\n",
    "    data_dim = X.shape[1]\n",
    "    num_classes = len(set(y))\n",
    "\n",
    "    # scale X\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    X = scaler.fit_transform(X)  # X*scaler.scale_+scaler.min_ (columnwise)\n",
    "    print('Multiplying each row in X elementwise: {}'.format(scaler.scale_))\n",
    "    print('Increasing each row in X elemtwise: {}'.format(scaler.min_))\n",
    "\n",
    "    # encode Y\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y)\n",
    "    encoded_y = encoder.transform(y)  # encoder.classes_\n",
    "    print('Hotencoding Y: {}'.format(encoder.classes_))\n",
    "    hot_encoded_y = np_utils.to_categorical(encoded_y)\n",
    "\n",
    "    # prepare data for LSTM\n",
    "    def create_LSTM_dataset(x, y, timesteps):\n",
    "        dataX, dataY = [], []\n",
    "        for i in range(len(x) - timesteps + 1):\n",
    "            dataX.append(x[i:i + timesteps, :])\n",
    "            dataY.append(y[i + timesteps - timesteps_in_future - 1, :])\n",
    "        return array(dataX), array(dataY)\n",
    "\n",
    "    X, hot_encoded_y = create_LSTM_dataset(X, hot_encoded_y, timesteps)\n",
    "\n",
    "    # define model\n",
    "    model = Sequential([\n",
    "        Conv1D(nodes_per_layer, filter_length, strides=2, activation='relu', input_shape=(timesteps, data_dim),\n",
    "               name='accelerations'),\n",
    "        Conv1D(nodes_per_layer, filter_length, strides=1, activation='relu'),\n",
    "        LSTM(nodes_per_layer, return_sequences=True),\n",
    "        LSTM(nodes_per_layer, return_sequences=False),\n",
    "        Dropout(dropout),\n",
    "        #Flatten(),\n",
    "        Dense(num_classes),\n",
    "        Activation('softmax', name='scores'),\n",
    "    ])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # compile model\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # define callbacks\n",
    "    callbacks = []\n",
    "\n",
    "    tensor_board = TensorBoard(log_dir=logs_path, histogram_freq=1, write_graph=False, write_images=False)\n",
    "    callbacks.append(tensor_board)\n",
    "\n",
    "    checkpoint_path = 'best_weights.h5'\n",
    "    checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks.append(checkpoint)\n",
    "\n",
    "    # train model\n",
    "    model.fit(X, hot_encoded_y,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_split=validation_split,\n",
    "              callbacks=callbacks\n",
    "              )\n",
    "\n",
    "    # load best checkpoint\n",
    "    model.load_weights('best_weights.h5')\n",
    "\n",
    "    # evaluate best model\n",
    "    def non_shuffling_train_test_split(X, y, test_size=validation_split):\n",
    "        i = int((1 - test_size) * X.shape[0]) + 1\n",
    "        X_train, X_test = split(X, [i])\n",
    "        y_train, y_test = split(y, [i])\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    _, X_test, _, y_test = non_shuffling_train_test_split(X, hot_encoded_y, test_size=validation_split)\n",
    "\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    acc = scores[1]\n",
    "\n",
    "    # save model\n",
    "    model_h5_name = 'model_acc_' + str(acc) + '.h5'\n",
    "    model.save(model_h5_name)\n",
    "\n",
    "    # save model.h5 on to google storage\n",
    "    with file_io.FileIO(model_h5_name, mode='r') as input_f:\n",
    "        with file_io.FileIO(logs_path + '/' + model_h5_name, mode='w+') as output_f:\n",
    "            output_f.write(input_f.read())\n",
    "\n",
    "            # reset session\n",
    "            # Note: If this piece of code did help you to achieve your goal, please upvote my solution under:\n",
    "            # https://stackoverflow.com/questions/41959318/deploying-keras-models-via-google-cloud-ml/44232441#44232441\n",
    "            # Thank you so much :)\n",
    "    k.clear_session()\n",
    "    sess = tf.Session()\n",
    "    k.set_session(sess)\n",
    "\n",
    "    # disable loading of learning nodes\n",
    "    k.set_learning_phase(0)\n",
    "\n",
    "    # load model\n",
    "    model = load_model(model_h5_name)\n",
    "    config = model.get_config()\n",
    "    weights = model.get_weights()\n",
    "    new_Model = Sequential.from_config(config)\n",
    "    new_Model.set_weights(weights)\n",
    "\n",
    "    # export coreml model\n",
    "\n",
    "    coreml_model = coremltools.converters.keras.convert(new_Model, input_names=['accelerations'],\n",
    "                                                        output_names=['scores'])\n",
    "    model_mlmodel_name = 'model_acc_' + str(acc) + '.mlmodel'\n",
    "    coreml_model.save(model_mlmodel_name)\n",
    "\n",
    "    # save model.mlmodel on to google storage\n",
    "    with file_io.FileIO(model_mlmodel_name, mode='r') as input_f:\n",
    "        with file_io.FileIO(logs_path + '/' + model_mlmodel_name, mode='w+') as output_f:\n",
    "            output_f.write(input_f.read())\n",
    "\n",
    "            # export saved model\n",
    "            # Note: If this piece of code did help you to achieve your goal, please upvote my solution under:\n",
    "            # https://stackoverflow.com/questions/41959318/deploying-keras-models-via-google-cloud-ml/44232441#44232441\n",
    "            # Thank you so much :)\n",
    "    export_path = logs_path + \"/export\"\n",
    "    builder = saved_model_builder.SavedModelBuilder(export_path)\n",
    "\n",
    "    signature = predict_signature_def(inputs={'accelerations': new_Model.input},\n",
    "                                      outputs={'scores': new_Model.output})\n",
    "\n",
    "    with k.get_session() as sess:\n",
    "        builder.add_meta_graph_and_variables(sess=sess,\n",
    "                                             tags=[tag_constants.SERVING],\n",
    "                                             signature_def_map={\n",
    "                                                 signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature})\n",
    "        builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "Using train_file located at ../MoviLabData/ExermoteFormatExerciseData.csv\n",
      "Using logs_path located at ./logs/final_25_classes_4_squats_adjusted_dropout_0.2_timesteps_40_timesteps_in_future_20_nodes_per_layer_32_filter_length_3\n",
      "-----------------------\n",
      "Multiplying each row in X elementwise: [0.48527931 0.38469124 0.47923248 0.00080302 0.00086807 0.00075656]\n",
      "Increasing each row in X elemtwise: [0.62413364 0.51796196 0.63402363 0.19779171 0.13280402 0.53725146]\n",
      "Hotencoding Y: ['break' 'pushup' 'raise' 'sqaut' 'squat']\n",
      "WARNING:tensorflow:From /home/dldustn14/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/dldustn14/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/dldustn14/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/dldustn14/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/dldustn14/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "accelerations (Conv1D)       (None, 19, 32)            608       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 17, 32)            3104      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 17, 32)            8320      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 165       \n",
      "_________________________________________________________________\n",
      "scores (Activation)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 20,517\n",
      "Trainable params: 20,517\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/dldustn14/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/dldustn14/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/dldustn14/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 24652 samples, validate on 6164 samples\n",
      "WARNING:tensorflow:From /home/dldustn14/anaconda3/lib/python3.7/site-packages/keras/callbacks.py:796: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/dldustn14/anaconda3/lib/python3.7/site-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/dldustn14/anaconda3/lib/python3.7/site-packages/keras/callbacks.py:856: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/50\n",
      " 8400/24652 [=========>....................] - ETA: 8s - loss: 1.1401 - acc: 0.4805"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-955ba77b3b52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \"\"\"\n\u001b[1;32m     19\u001b[0m     \u001b[0marguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'train_file'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'../MoviLabData/ExermoteFormatExerciseData.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'job_dir'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'./'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-29a6147d8ad1>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_file, job_dir, **args)\u001b[0m\n\u001b[1;32m     93\u001b[0m               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m               \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m               )\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Input Arguments\n",
    "    parser.add_argument(\n",
    "        '--train-file',\n",
    "        help='GCS or local paths to training data',\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help='GCS location to write checkpoints and export models',\n",
    "        required=True\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "    print('trainfile & job dir arguments :',arguments)\n",
    "    \"\"\"\n",
    "    arguments = {'train_file': '../MoviLabData/ExermoteFormatExerciseData.csv', 'job_dir': './'}\n",
    "    train_model(**arguments)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
