{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.lib.io import file_io\n",
    "import argparse\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM, Conv1D, Flatten\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from numpy import array, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as k\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from tensorflow.python.saved_model import builder as saved_model_builder\n",
    "from tensorflow.python.saved_model import tag_constants, signature_constants\n",
    "from tensorflow.python.saved_model.signature_def_utils_impl import predict_signature_def\n",
    "\n",
    "import coremltools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "epochs = 50\n",
    "\n",
    "batch_size = 80\n",
    "validation_split = 0.2\n",
    "\n",
    "# model parameters\n",
    "dropout = 0.2\n",
    "timesteps = 40\n",
    "timesteps_in_future = 20\n",
    "nodes_per_layer = 32\n",
    "filter_length = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_file='data_classes_4_squats_adjusted.csv', job_dir='leeeeeroooooyyyyyjeeeeeenkins', **args):\n",
    "    parameter_string = 'final_25_classes_4_squats_adjusted' + '_dropout_' + str(dropout) + '_timesteps_' + str(\n",
    "        timesteps) + '_timesteps_in_future_' + str(timesteps_in_future) + '_nodes_per_layer_' + str(\n",
    "        nodes_per_layer) + '_filter_length_' + str(filter_length)\n",
    "    if 'gs://' in job_dir:\n",
    "        logs_path = 'gs://exermotemachinelearningengine' + '/logs/' + parameter_string\n",
    "    else:\n",
    "        logs_path = '.' + '/logs/' + parameter_string\n",
    "    print('-----------------------')\n",
    "    print('Using train_file located at {}'.format(train_file))\n",
    "    print('Using logs_path located at {}'.format(logs_path))\n",
    "    print('-----------------------')\n",
    "\n",
    "    # load data\n",
    "    file_stream = file_io.FileIO(train_file, mode='r')\n",
    "    dataframe = read_csv(file_stream, header=0)\n",
    "    dataframe.fillna(0, inplace=True)\n",
    "    dataset = dataframe.values\n",
    "\n",
    "    X = dataset[:, [\n",
    "        #2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, # Device: xGravity, yGravity, zGravity, xAcceleration, yAcceleration, zAcceleration, pitch, roll, yaw, xRotationRate, yRotationRate, zRotationRate\n",
    "        2, 3, 4, 5, 6, 7, 8, 9, 10, 11  # xAcceleration, yAcceleration, zAcceleration, xRotationRate, yRotationRate, zRotationRate\n",
    "        # 14,15,16,17,                          # Right Hand: rssi, xAcceleration, yAcceleration, zAcceleration\n",
    "        # 18,19,20,21,                          # Left Hand: rssi, xAcceleration, yAcceleration, zAcceleration\n",
    "        # 22,23,24,25,                          # Right Foot: rssi, xAcceleration, yAcceleration, zAcceleration\n",
    "        # 26,27,28,29,                          # Left Foot: rssi, xAcceleration, yAcceleration, zAcceleration\n",
    "        # 30,31,32,33,                          # Chest: rssi, xAcceleration, yAcceleration, zAcceleration\n",
    "        # 34,35,36,37                           # Belly: rssi, xAcceleration, yAcceleration, zAcceleration\n",
    "    ]].astype(float)\n",
    "    y = dataset[:, 0]  # ExerciseType (Index 1 is ExerciseSubType)\n",
    "\n",
    "    # data parameters\n",
    "    data_dim = X.shape[1]\n",
    "    num_classes = len(set(y))\n",
    "\n",
    "    # scale X\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    X = scaler.fit_transform(X)  # X*scaler.scale_+scaler.min_ (columnwise)\n",
    "    print('Multiplying each row in X elementwise: {}'.format(scaler.scale_))\n",
    "    print('Increasing each row in X elemtwise: {}'.format(scaler.min_))\n",
    "\n",
    "    # encode Y\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y)\n",
    "    encoded_y = encoder.transform(y)  # encoder.classes_\n",
    "    print('Hotencoding Y: {}'.format(encoder.classes_))\n",
    "    hot_encoded_y = np_utils.to_categorical(encoded_y)\n",
    "\n",
    "    # prepare data for LSTM\n",
    "    def create_LSTM_dataset(x, y, timesteps):\n",
    "        dataX, dataY = [], []\n",
    "        for i in range(len(x) - timesteps + 1):\n",
    "            dataX.append(x[i:i + timesteps, :])\n",
    "            dataY.append(y[i + timesteps - timesteps_in_future - 1, :])\n",
    "        return array(dataX), array(dataY)\n",
    "\n",
    "    X, hot_encoded_y = create_LSTM_dataset(X, hot_encoded_y, timesteps)\n",
    "\n",
    "    # define model\n",
    "    model = Sequential([\n",
    "        Conv1D(nodes_per_layer, filter_length, strides=2, activation='relu', input_shape=(timesteps, data_dim),\n",
    "               name='accelerations'),\n",
    "        Conv1D(nodes_per_layer, filter_length, strides=1, activation='relu'),\n",
    "        LSTM(nodes_per_layer, return_sequences=True),\n",
    "        LSTM(nodes_per_layer, return_sequences=False),\n",
    "        Dropout(dropout),\n",
    "        #Flatten(),\n",
    "        Dense(num_classes),\n",
    "        Activation('softmax', name='scores'),\n",
    "    ])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # compile model\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # define callbacks\n",
    "    callbacks = []\n",
    "\n",
    "    tensor_board = TensorBoard(log_dir=logs_path, histogram_freq=1, write_graph=False, write_images=False)\n",
    "    callbacks.append(tensor_board)\n",
    "\n",
    "    checkpoint_path = 'best_weights.h5'\n",
    "    checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks.append(checkpoint)\n",
    "\n",
    "    # train model\n",
    "    model.fit(X, hot_encoded_y,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_split=validation_split,\n",
    "              callbacks=callbacks\n",
    "              )\n",
    "\n",
    "    # load best checkpoint\n",
    "    model.load_weights('best_weights.h5')\n",
    "\n",
    "    # evaluate best model\n",
    "    def non_shuffling_train_test_split(X, y, test_size=validation_split):\n",
    "        i = int((1 - test_size) * X.shape[0]) + 1\n",
    "        X_train, X_test = split(X, [i])\n",
    "        y_train, y_test = split(y, [i])\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    _, X_test, _, y_test = non_shuffling_train_test_split(X, hot_encoded_y, test_size=validation_split)\n",
    "\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    acc = scores[1]\n",
    "\n",
    "    # save model\n",
    "    model_h5_name = 'model_acc_' + str(acc) + '.h5'\n",
    "    model.save(model_h5_name)\n",
    "\n",
    "    # save model.h5 on to google storage\n",
    "    with file_io.FileIO(model_h5_name, mode='r') as input_f:\n",
    "        with file_io.FileIO(logs_path + '/' + model_h5_name, mode='w+') as output_f:\n",
    "            output_f.write(input_f.read())\n",
    "\n",
    "            # reset session\n",
    "            # Note: If this piece of code did help you to achieve your goal, please upvote my solution under:\n",
    "            # https://stackoverflow.com/questions/41959318/deploying-keras-models-via-google-cloud-ml/44232441#44232441\n",
    "            # Thank you so much :)\n",
    "    k.clear_session()\n",
    "    sess = tf.Session()\n",
    "    k.set_session(sess)\n",
    "\n",
    "    # disable loading of learning nodes\n",
    "    k.set_learning_phase(0)\n",
    "\n",
    "    # load model\n",
    "    model = load_model(model_h5_name)\n",
    "    config = model.get_config()\n",
    "    weights = model.get_weights()\n",
    "    new_Model = Sequential.from_config(config)\n",
    "    new_Model.set_weights(weights)\n",
    "\n",
    "    # export coreml model\n",
    "\n",
    "    coreml_model = coremltools.converters.keras.convert(new_Model, input_names=['accelerations'],\n",
    "                                                        output_names=['scores'])\n",
    "    model_mlmodel_name = 'model_acc_' + str(acc) + '.mlmodel'\n",
    "    coreml_model.save(model_mlmodel_name)\n",
    "\n",
    "    # save model.mlmodel on to google storage\n",
    "    with file_io.FileIO(model_mlmodel_name, mode='r') as input_f:\n",
    "        with file_io.FileIO(logs_path + '/' + model_mlmodel_name, mode='w+') as output_f:\n",
    "            output_f.write(input_f.read())\n",
    "\n",
    "            # export saved model\n",
    "            # Note: If this piece of code did help you to achieve your goal, please upvote my solution under:\n",
    "            # https://stackoverflow.com/questions/41959318/deploying-keras-models-via-google-cloud-ml/44232441#44232441\n",
    "            # Thank you so much :)\n",
    "    export_path = logs_path + \"/export\"\n",
    "    builder = saved_model_builder.SavedModelBuilder(export_path)\n",
    "\n",
    "    signature = predict_signature_def(inputs={'accelerations': new_Model.input},\n",
    "                                      outputs={'scores': new_Model.output})\n",
    "\n",
    "    with k.get_session() as sess:\n",
    "        builder.add_meta_graph_and_variables(sess=sess,\n",
    "                                             tags=[tag_constants.SERVING],\n",
    "                                             signature_def_map={\n",
    "                                                 signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature})\n",
    "        builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "Using train_file located at ../MoviLabData/Learning_xin_jaeyeong_minzy_dagon_situp.csv\n",
      "Using logs_path located at ./logs/final_25_classes_4_squats_adjusted_dropout_0.2_timesteps_40_timesteps_in_future_20_nodes_per_layer_32_filter_length_3\n",
      "-----------------------\n",
      "Multiplying each row in X elementwise: [0.38933511 0.33194214 0.48995215 0.00242604 0.00487805 0.00225244\n",
      " 0.66597661 0.00416425 0.00298891 0.00298891]\n",
      "Increasing each row in X elemtwise: [ 0.54978376  0.53523238  0.46907895  0.47241124  0.51903629  0.46909765\n",
      " -0.3082647  -0.00189164  0.99530543  0.9953054 ]\n",
      "Hotencoding Y: ['pushup' 'situp' 'squat']\n",
      "WARNING:tensorflow:From /home/dldustn14/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/dldustn14/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/dldustn14/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/dldustn14/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/dldustn14/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "accelerations (Conv1D)       (None, 19, 32)            992       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 17, 32)            3104      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 17, 32)            8320      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 99        \n",
      "_________________________________________________________________\n",
      "scores (Activation)          (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 20,835\n",
      "Trainable params: 20,835\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/dldustn14/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/dldustn14/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/dldustn14/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 27102 samples, validate on 6776 samples\n",
      "WARNING:tensorflow:From /home/dldustn14/anaconda3/lib/python3.7/site-packages/keras/callbacks.py:796: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/dldustn14/anaconda3/lib/python3.7/site-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/dldustn14/anaconda3/lib/python3.7/site-packages/keras/callbacks.py:856: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/50\n",
      "27102/27102 [==============================] - 12s 434us/step - loss: 0.3993 - acc: 0.8059 - val_loss: 0.7677 - val_acc: 0.5007\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.50074, saving model to best_weights.h5\n",
      "Epoch 2/50\n",
      "27102/27102 [==============================] - 10s 381us/step - loss: 0.1676 - acc: 0.9506 - val_loss: 0.0989 - val_acc: 0.9692\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.50074 to 0.96916, saving model to best_weights.h5\n",
      "Epoch 3/50\n",
      "27102/27102 [==============================] - 10s 380us/step - loss: 0.0772 - acc: 0.9790 - val_loss: 0.1369 - val_acc: 0.9699\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.96916 to 0.96989, saving model to best_weights.h5\n",
      "Epoch 4/50\n",
      "27102/27102 [==============================] - 10s 382us/step - loss: 0.0728 - acc: 0.9811 - val_loss: 0.2270 - val_acc: 0.9491\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.96989\n",
      "Epoch 5/50\n",
      "27102/27102 [==============================] - 10s 381us/step - loss: 0.0592 - acc: 0.9855 - val_loss: 0.2863 - val_acc: 0.9345\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.96989\n",
      "Epoch 6/50\n",
      "27102/27102 [==============================] - 10s 379us/step - loss: 0.0366 - acc: 0.9904 - val_loss: 2.2706 - val_acc: 0.4783\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.96989\n",
      "Epoch 7/50\n",
      "27102/27102 [==============================] - 10s 378us/step - loss: 0.0351 - acc: 0.9913 - val_loss: 0.2350 - val_acc: 0.9569\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.96989\n",
      "Epoch 8/50\n",
      "27102/27102 [==============================] - 10s 382us/step - loss: 0.0395 - acc: 0.9906 - val_loss: 0.2160 - val_acc: 0.9604\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.96989\n",
      "Epoch 9/50\n",
      "27102/27102 [==============================] - 10s 386us/step - loss: 0.0259 - acc: 0.9914 - val_loss: 0.2324 - val_acc: 0.9541\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.96989\n",
      "Epoch 10/50\n",
      "27102/27102 [==============================] - 10s 383us/step - loss: 0.0286 - acc: 0.9921 - val_loss: 0.2814 - val_acc: 0.9442\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.96989\n",
      "Epoch 11/50\n",
      "27102/27102 [==============================] - 10s 384us/step - loss: 0.0192 - acc: 0.9934 - val_loss: 0.2254 - val_acc: 0.9556\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.96989\n",
      "Epoch 12/50\n",
      "27102/27102 [==============================] - 10s 380us/step - loss: 0.0228 - acc: 0.9931 - val_loss: 0.1409 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.96989 to 0.97919, saving model to best_weights.h5\n",
      "Epoch 13/50\n",
      "27102/27102 [==============================] - 10s 381us/step - loss: 0.0195 - acc: 0.9946 - val_loss: 0.1910 - val_acc: 0.9717\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.97919\n",
      "Epoch 14/50\n",
      "27102/27102 [==============================] - 10s 381us/step - loss: 0.0218 - acc: 0.9954 - val_loss: 0.2425 - val_acc: 0.9618\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.97919\n",
      "Epoch 15/50\n",
      "27102/27102 [==============================] - 10s 381us/step - loss: 0.0177 - acc: 0.9957 - val_loss: 0.1946 - val_acc: 0.9684\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.97919\n",
      "Epoch 16/50\n",
      "27102/27102 [==============================] - 10s 380us/step - loss: 0.0146 - acc: 0.9955 - val_loss: 0.2062 - val_acc: 0.9699\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.97919\n",
      "Epoch 17/50\n",
      "27102/27102 [==============================] - 10s 382us/step - loss: 0.0205 - acc: 0.9948 - val_loss: 0.2156 - val_acc: 0.9662\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.97919\n",
      "Epoch 18/50\n",
      "27102/27102 [==============================] - 10s 381us/step - loss: 0.0148 - acc: 0.9968 - val_loss: 0.2236 - val_acc: 0.9693\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.97919\n",
      "Epoch 19/50\n",
      "27102/27102 [==============================] - 10s 382us/step - loss: 0.0174 - acc: 0.9960 - val_loss: 0.1984 - val_acc: 0.9693\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.97919\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27102/27102 [==============================] - 10s 385us/step - loss: 0.0139 - acc: 0.9965 - val_loss: 0.1576 - val_acc: 0.9782\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.97919\n",
      "Epoch 21/50\n",
      "27102/27102 [==============================] - 10s 381us/step - loss: 0.0206 - acc: 0.9956 - val_loss: 0.1969 - val_acc: 0.9727\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.97919\n",
      "Epoch 22/50\n",
      "27102/27102 [==============================] - 10s 380us/step - loss: 0.0169 - acc: 0.9961 - val_loss: 0.2142 - val_acc: 0.9693\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.97919\n",
      "Epoch 23/50\n",
      "27102/27102 [==============================] - 10s 381us/step - loss: 0.0148 - acc: 0.9967 - val_loss: 0.4569 - val_acc: 0.9305\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.97919\n",
      "Epoch 24/50\n",
      "27102/27102 [==============================] - 10s 382us/step - loss: 0.0176 - acc: 0.9968 - val_loss: 0.3770 - val_acc: 0.9371\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.97919\n",
      "Epoch 25/50\n",
      "27102/27102 [==============================] - 10s 380us/step - loss: 0.0208 - acc: 0.9971 - val_loss: 0.1761 - val_acc: 0.9774\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.97919\n",
      "Epoch 26/50\n",
      "27102/27102 [==============================] - 10s 383us/step - loss: 0.0213 - acc: 0.9962 - val_loss: 0.1648 - val_acc: 0.9780\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.97919\n",
      "Epoch 27/50\n",
      "27102/27102 [==============================] - 10s 382us/step - loss: 0.0213 - acc: 0.9952 - val_loss: 0.2209 - val_acc: 0.9671\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.97919\n",
      "Epoch 28/50\n",
      "27102/27102 [==============================] - 10s 383us/step - loss: 0.0128 - acc: 0.9974 - val_loss: 0.1935 - val_acc: 0.9721\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.97919\n",
      "Epoch 29/50\n",
      "27102/27102 [==============================] - 10s 384us/step - loss: 0.0108 - acc: 0.9972 - val_loss: 0.1786 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.97919\n",
      "Epoch 30/50\n",
      "27102/27102 [==============================] - 10s 384us/step - loss: 0.0128 - acc: 0.9965 - val_loss: 0.3452 - val_acc: 0.9464\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.97919\n",
      "Epoch 31/50\n",
      "27102/27102 [==============================] - 10s 380us/step - loss: 0.0184 - acc: 0.9966 - val_loss: 0.1885 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.97919\n",
      "Epoch 32/50\n",
      "27102/27102 [==============================] - 10s 381us/step - loss: 0.0212 - acc: 0.9966 - val_loss: 0.1557 - val_acc: 0.9752\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.97919\n",
      "Epoch 33/50\n",
      "27102/27102 [==============================] - 10s 378us/step - loss: 0.0181 - acc: 0.9974 - val_loss: 0.2140 - val_acc: 0.9681\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.97919\n",
      "Epoch 34/50\n",
      "27102/27102 [==============================] - 10s 381us/step - loss: 0.0190 - acc: 0.9971 - val_loss: 0.2519 - val_acc: 0.9664\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.97919\n",
      "Epoch 35/50\n",
      "27102/27102 [==============================] - 10s 379us/step - loss: 0.0174 - acc: 0.9972 - val_loss: 0.2065 - val_acc: 0.9693\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.97919\n",
      "Epoch 36/50\n",
      "27102/27102 [==============================] - 10s 382us/step - loss: 0.0129 - acc: 0.9980 - val_loss: 0.1710 - val_acc: 0.9677\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.97919\n",
      "Epoch 37/50\n",
      "27102/27102 [==============================] - 11s 388us/step - loss: 0.0098 - acc: 0.9976 - val_loss: 0.2218 - val_acc: 0.9675\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.97919\n",
      "Epoch 38/50\n",
      "27102/27102 [==============================] - 10s 378us/step - loss: 0.0129 - acc: 0.9970 - val_loss: 0.2620 - val_acc: 0.9584\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.97919\n",
      "Epoch 39/50\n",
      "27102/27102 [==============================] - 10s 378us/step - loss: 0.0138 - acc: 0.9975 - val_loss: 0.1364 - val_acc: 0.9758\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.97919\n",
      "Epoch 40/50\n",
      "27102/27102 [==============================] - 10s 383us/step - loss: 0.0065 - acc: 0.9982 - val_loss: 0.2410 - val_acc: 0.9684\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.97919\n",
      "Epoch 41/50\n",
      "27102/27102 [==============================] - 10s 380us/step - loss: 0.0131 - acc: 0.9974 - val_loss: 0.2275 - val_acc: 0.9689\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.97919\n",
      "Epoch 42/50\n",
      "27102/27102 [==============================] - 10s 384us/step - loss: 0.0159 - acc: 0.9979 - val_loss: 0.2397 - val_acc: 0.9640\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.97919\n",
      "Epoch 43/50\n",
      "27102/27102 [==============================] - 10s 383us/step - loss: 0.0089 - acc: 0.9982 - val_loss: 0.2012 - val_acc: 0.9728\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.97919\n",
      "Epoch 44/50\n",
      "27102/27102 [==============================] - 10s 384us/step - loss: 0.0066 - acc: 0.9986 - val_loss: 0.2780 - val_acc: 0.9599\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.97919\n",
      "Epoch 45/50\n",
      "27102/27102 [==============================] - 16s 598us/step - loss: 0.0119 - acc: 0.9977 - val_loss: 0.2856 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.97919\n",
      "Epoch 46/50\n",
      "27102/27102 [==============================] - 17s 635us/step - loss: 0.0128 - acc: 0.9978 - val_loss: 0.2500 - val_acc: 0.9624\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.97919\n",
      "Epoch 47/50\n",
      "27102/27102 [==============================] - 12s 439us/step - loss: 0.0082 - acc: 0.9985 - val_loss: 0.3060 - val_acc: 0.9584\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.97919\n",
      "Epoch 48/50\n",
      "27102/27102 [==============================] - 22s 795us/step - loss: 0.0085 - acc: 0.9986 - val_loss: 0.2064 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.97919\n",
      "Epoch 49/50\n",
      "27102/27102 [==============================] - 21s 779us/step - loss: 0.0123 - acc: 0.9977 - val_loss: 0.2219 - val_acc: 0.9684\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.97919\n",
      "Epoch 50/50\n",
      "27102/27102 [==============================] - 10s 380us/step - loss: 0.0096 - acc: 0.9983 - val_loss: 0.2132 - val_acc: 0.9696\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.97919\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b2b26ec95508>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \"\"\"\n\u001b[1;32m     19\u001b[0m     \u001b[0marguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'train_file'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'../MoviLabData/Learning_xin_jaeyeong_minzy_dagon_situp.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'job_dir'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'./'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-8ae591c97172>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_file, job_dir, **args)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_h5_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minput_f\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_h5_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutput_f\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0moutput_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;31m# reset session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    126\u001b[0m       \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     return self._prepare_value(\n\u001b[0;32m--> 128\u001b[0;31m         pywrap_tensorflow.ReadFromStream(self._read_buf, length))\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m   @deprecation.deprecated_args(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m_prepare_value\u001b[0;34m(self, val)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/compat.py\u001b[0m in \u001b[0;36mas_str_any\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    115\u001b[0m   \"\"\"\n\u001b[1;32m    116\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/compat.py\u001b[0m in \u001b[0;36mas_text\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbytes_or_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_or_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbytes_or_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expected binary or unicode string, got %r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbytes_or_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Input Arguments\n",
    "    parser.add_argument(\n",
    "        '--train-file',\n",
    "        help='GCS or local paths to training data',\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help='GCS location to write checkpoints and export models',\n",
    "        required=True\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "    print('trainfile & job dir arguments :',arguments)\n",
    "    \"\"\"\n",
    "    arguments = {'train_file': '../MoviLabData/Learning_xin_jaeyeong_minzy_dagon_situp.csv', 'job_dir': './'}\n",
    "    train_model(**arguments)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
